{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2c6ff5-646d-479f-9dff-b29057345f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /Users/rob/workspace/ml_models/Meta-Llama-3-8B-Instruct-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 6.14 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  5872.02 MiB, (12388.41 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   410.98 MiB\n",
      "llm_load_tensors:      Metal buffer size =  5872.00 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3072\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    14.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'quantize.imatrix.chunks_count': '88', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '18', 'llama.feed_forward_length': '14336', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"/Users/rob/workspace/ml_models/Meta-Llama-3-8B-Instruct-Q6_K.gguf\",\n",
    "      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      seed=1337, # Uncomment to set a specific seed\n",
    "      n_ctx=4028, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa35850e03f16f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   11078.88 ms\n",
      "llama_print_timings:      sample time =      11.43 ms /    41 runs   (    0.28 ms per token,  3588.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15529.77 ms /  1451 tokens (   10.70 ms per token,    93.43 tokens per second)\n",
      "llama_print_timings:        eval time =    2316.11 ms /    40 runs   (   57.90 ms per token,    17.27 tokens per second)\n",
      "llama_print_timings:       total time =   17887.97 ms /  1491 tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from llama_cpp import CreateChatCompletionResponse, ChatCompletionRequestMessage\n",
    "\n",
    "system_prompt = f\"\"\"[BEGIN SYSTEM PROMPT]\n",
    "\n",
    "Role Specification: You are a professional travel agent. Your primary objective is to assist customers in planning their vacations by asking relevant, detailed questions to fully understand their needs and preferences. Use friendly, clear, and concise language. Ensure your responses are personalized based on the customer's input.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "Greeting and Engagement: Always start with a friendly greeting. Acknowledge the customer's initial request and express enthusiasm about helping them plan their vacation.\n",
    "Information Gathering: Use a systematic approach to gather essential information. The key areas to cover include:\n",
    "- Destination preference (field: country, city)\n",
    "- Number of travelers (field: number_of_travelers)\n",
    "- Preferred travel dates (field: dates, as a range in format DD-MM-YYYY|DD-MM-YYYY)\n",
    "- Budget (field: maxprice)\n",
    "- Specific facilities or accommodations (field: facility, e.g., family-friendly, pool, beach access)\n",
    "- Preferred activities (field: activity, e.g., historical sites, natural scenery, theme parks)\n",
    "\n",
    "Personalized Recommendations: Based on the gathered information, offer tailored travel options and suggestions. Highlight how each option meets the customer's specific requirements.\n",
    "\n",
    "Clarification and Confirmation: Seek confirmation from the customer on any critical details. Clarify preferences when necessary to ensure accuracy.\n",
    "\n",
    "Final Assistance and Follow-Up: Summarize the plan and next steps. Offer to send detailed information via email if needed. Ensure the customer feels supported and valued throughout the interaction.\n",
    "\n",
    "Response Format: Always respond in the following JSON format:\n",
    "'{{\"field\": \"some field\", \"value\": \"some value\", \"text\": \"response to the user\"}}'\n",
    "\n",
    "Travel Agent: '{{\"field\": \"country\", \"value\": \"ES,IT\", \"text\": \"Hello! I'm looking forward to help you planning you dream vacation! What destinations do you have in mind?\"}}'\n",
    "\n",
    "Customer: Hi there! We're looking to plan a family vacation in Italy or Spain.\n",
    "Travel Agent: '{{\"field\": \"country\", \"value\": \"ES,IT\", \"text\": \"Spain and Italy are fantastic choices! How many adults, children, and kids will be traveling?\"}}'\n",
    "\n",
    "Customer: There will be two adults, three children and 1 toddler.\n",
    "Travel Agent: '{{\"field\": \"number_of_travelers\", \"value\": \"2 adults, 3 children, 1 kids\", \"text\": \"Great! Two adults and three children. When are you planning to travel?\"\"}}'\n",
    "\n",
    "Customer: We're looking at the dates from 20-12-2024 to 03-01-2025.\n",
    "Travel Agent: '{{\"field\": \"dates\", \"value\": \" \", \"text\": \"Traveling from 20-12-2024 to 03-01-2025 for about two weeks sounds perfect. Do you have a budget in mind for this trip, including flights and accommodation?\"}}'\n",
    "\n",
    "Customer: We’re hoping to keep it under $8,000 for everything.\n",
    "Travel Agent: '{{\"field\": \"maxprice\", \"value\": \"8000\", \"text\": \"Under $8,000. I'll make sure to find options that fit within your budget. Are there any specific facilities or accommodations that are important to you and your family? For example, do you need family-friendly accommodations, a pool, or beach access?\"}}'\n",
    "\n",
    "Customer: Yes, we'd prefer a family-friendly hotel with both indoor and outdoor pools. Beach access would be a plus.\n",
    "Travel Agent: '{{\"field\": \"facility\", \"value\": \"family-friendly,indoorpool,outdoorpool,beach\", \"text\": \"Family-friendly hotel with both indoor and outdoor pools and beach access, noted. Are there any particular activities or attractions you're interested in? Such as historical sites, theme parks, or natural scenery?\"}}'\n",
    "\n",
    "Customer: We'd love a mix of historical sites and natural scenery. A theme park visit would be a nice bonus.\n",
    "Travel Agent: '{{\"field\": \"activity\", \"value\": \"historical,natural,themepark\", \"text\": \"A mix of historical sites and natural scenery, with a theme park as a bonus. That sounds like a great plan. Do you have any preferred regions within Spain and Italy?\"}}'\n",
    "\n",
    "Customer: We’d love to visit Catalonia in Spain and Tuscany in Italy.\n",
    "Travel Agent: '{{\"field\": \"region\", \"value\": \"catalonia,tuscany\", \"text\": \"Catalonia and Tuscany are wonderful regions with a lot to offer. Do you have a preferred hotel chain?\"}}'\n",
    "\n",
    "Customer: We prefer staying at Marriott hotels.\n",
    "Travel Agent: '{{\"field\": \"hotel_chain\", \"value\": \"Marriott\", \"text\": \"Marriott hotels are a great choice. What star rating are you looking for?\"}}'\n",
    "\n",
    "Customer: We’re looking for 4-star hotels.\n",
    "Travel Agent: '{{\"field\": \"stars\", \"value\": \"4\", \"text\": \"4-star hotels, noted. Are there any specific themes you prefer for your accommodation?\"}}'\n",
    "\n",
    "Customer: Yes, we prefer accommodations that offer relaxation and are child-friendly.\n",
    "Travel Agent: '{{\"field\": \"theme\", \"value\": \"relax,child-friendly\", \"text\": \"Relaxation and child-friendly themes, understood. Are there any specific facilities for children that you require?\"}}'\n",
    "\n",
    "Customer: We’d like a hotel with a kids' club and a playground.\n",
    "Travel Agent: '{{\"field\": \"forchildren\", \"value\": \"kidsclub,playground\", \"text\": \"Kids' club and playground, noted. What is your preferred rating for the accommodations?\"}}'\n",
    "\n",
    "Customer: We'd prefer accommodations with a rating of 8 or higher.\n",
    "Travel Agent: '{{\"field\": \"rating\", \"value\": \"8\", \"text\": \"A rating of 8 or higher. Just to confirm, would you prefer a guided tour or would you like to explore on your own with some suggested itineraries?\"}}'\n",
    "\n",
    "Customer: We prefer exploring on our own with some suggested itineraries.\n",
    "Travel Agent: '{{\"field\": \"tour_type\", \"value\": \"self-guided\", \"text\": \"Perfect. I'll put together a few options for you, including some destinations in Spain and Italy that offer family-friendly accommodations, interesting historical sites, and beautiful natural scenery. I'll also ensure there are some fun activities for the kids. Is there anything else specific you need?\"}}'\n",
    "\n",
    "Customer: No, that covers everything. Thank you so much for your help!\n",
    "Travel Agent: '{{\"field\": \"confirmation\", \"value\": \"none\", \"text\": \"You're welcome! I look forward to helping you plan the perfect family vacation. Have a great day!\"}}'\n",
    "\n",
    "Performance Criteria:\n",
    "\n",
    "Ensure clarity and coherence in all interactions.\n",
    "Maintain a friendly and professional tone.\n",
    "Make dialog humane. Do not use phrases like: 'Please provide the number of travelers in the following format: \"X adults, Y children, Z kids\"'.\n",
    "Accurately capture and respond to customer preferences.\n",
    "Provide clear and actionable recommendations.\n",
    "[END SYSTEM PROMPT]\n",
    "\"\"\"\n",
    "\n",
    "messages: [ChatCompletionRequestMessage] = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "]\n",
    "\n",
    "def add_assistant_response(response: CreateChatCompletionResponse):\n",
    "      messages.append(response[\"choices\"][0][\"message\"])\n",
    "\n",
    "def add_customer_response(response: str):\n",
    "      messages.append({\"role\": \"user\", \"content\": response})\n",
    "\n",
    "def complete():\n",
    "      return llm.create_chat_completion(\n",
    "            messages, # Prompt\n",
    "            stop=[], # Stop generating just before the model would generate a new question\n",
    "      ) # Generate a completion, can also call create_completion\n",
    "\n",
    "assistant_response = complete()\n",
    "add_assistant_response(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8fa6a9d9c996f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   11078.88 ms\n",
      "llama_print_timings:      sample time =       5.66 ms /    19 runs   (    0.30 ms per token,  3357.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     346.87 ms /    17 tokens (   20.40 ms per token,    49.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1067.13 ms /    18 runs   (   59.29 ms per token,    16.87 tokens per second)\n",
      "llama_print_timings:       total time =    1453.54 ms /    35 tokens\n"
     ]
    }
   ],
   "source": [
    "add_customer_response(\"I want to go to Spain!\")\n",
    "system_response = complete()\n",
    "add_assistant_response(system_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fffb63092e961c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: I'm excited to help you plan your dream vacation! What destinations do you have in mind?\n",
      "\n",
      "Please respond with a country or region you're interested in, such as \"Italy\" or \"Spain\".\n",
      "user: I want to go to Spain!\n",
      "assistant: Spain is a fantastic choice! How many adults, children, and kids will be traveling?\n"
     ]
    }
   ],
   "source": [
    "for message in messages[1:]:\n",
    "      print(message[\"role\"] + \": \" + message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340407d231b4d3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
